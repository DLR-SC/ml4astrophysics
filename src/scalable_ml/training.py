from tqdm import tqdm
import numpy as np
import torch
import scalable_ml.model_utilities as util


def train_model(ml_model, device, train_loader, val_loader, epochs):
    """
    Trains a pytorch model

    For the optimizer we use the Adam algorithm
    - https://pytorch.org/docs/stable/generated/torch.optim.Adam.html
    - https://arxiv.org/abs/1412.6980

   For the loss function we use the mean squared error (squared L2 norm)
   - https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html

    :param ml_model:
    :type ml_model: torch Model object
    :param device: device on which training is performed
    :type device: basestring
    :param train_loader: training dataset
    :type train_loader: torch DataLoader
    :param epochs: number of epochs used for training
    :type epochs: int
    :param val_loader:  training validation set
    :type train_loader: torch DataLoader
    """
    params = [
        {'params': ml_model.parameters()}
    ]

    optimizer = torch.optim.Adam(params, lr=1e-3, weight_decay=1e-5)
    loss_function = torch.nn.CrossEntropyLoss()

    losses = {'train_loss': [], 'val_loss': []}
    accuracy = {'train_accuracy': [], 'val_accuracy': []}

    # loop for training
    for epoch in range(epochs):
        train_loss, train_accuracy = train_epoch(...)
        val_loss, val_accuracy = val_epoch(...)

        # print some information on loss decay and update dictionaries
        # Your code ...

    return losses, accuracy


def train_epoch(ml_model, device, dataloader, loss_fn, optimizer):
    """
    Performs one training epoch.

    :param ml_model: Model that is being trained
    :type ml_model: list of torch Model object
    :param device: device on which training is performed
    :type device: basestring
    :param dataloader: training dataset
    :type dataloader: torch DataLoader
    :param loss_fn: loss-function
    :type loss_fn: loss-function object
    :param optimizer: model optimizer
    :type optimizer: torch optimizer object
    :return: mean of training loss during epoch
    :rtype: float
    """

    train_loss = []
    train_accuracy = []

    # required since we shift model to eval model in accuracy function
    ml_model.train()  # sets model to train mode, i.e. activates dropout layers not used for inference

    # create progress bar
    loop = tqdm(dataloader)

    # iterate over batches generated by dataloader

    # Your Code ...

    return np.mean(train_loss), np.mean(train_accuracy)


def val_epoch(ml_model, device, val_loader, loss_fn):
    ml_model.eval()  # Set the model to evaluation mode, disabling dropout and using population
    # statistics for batch normalization.

    val_loss = []
    val_accuracy = []

    # with torch.no_grad():
    # iterate over batches generated by dataloader
    # Your Code ...

    return np.mean(val_loss), np.mean(val_accuracy)

